# Xpiler-eval: A Benchmark for Evaluating LLMs on Deep Learning Systems Programming

**Xpiler-eval** is a benchmark for testing large language models (LLMs) on **cross-system code translation** tasks in deep learning compilers.

It focuses on translating tensor operators between different hardware platforms, such as CUDA, CPU, and NPU.

## Evaluation Metrics

- **Compilation Success Rate**  
  Whether the generated code compiles correctly.

- **Execution Success Rate**  
  Whether the compiled code runs without errors.

- **Performance Comparison**  
  Runtime performance compared to vendor-provided library implementations.

